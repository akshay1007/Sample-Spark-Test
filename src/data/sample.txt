Background
Spark was born in 2009 in the laboratory AMPLab the University of Berkeley on the assumption that:

on one hand, RAM is becoming less expensive and therefore the servers have more RAM available
on the other hand, many data sets called “Big Data” data have a size of about 10 GB, so they can be held in memory
The project integrated the Apache Incubator in June 2013 and became a “Top-Level Project” in February 2014.

Spark version 1.0.0 was released in May 2014 and the project is now at version 1.1.0. It is undergoing rapid changes. The ecosystem now includes Spark and several tools:

Spark for “batch” processing
Spark Streaming for continuous processing of data stream
MLlib for “machine learning”
GraphX ​​for graph calculations (still in alpha)
Spark SQL, an implementation of SQL-like query data
Moreover, Spark integrates seamlessly with the Hadoop ecosystem (HDFS particular), and integrations with Cassandra and ElasticSearch are planned.
Finally, the framework is written in Scala and offers a Java binding that is easy to use. Java 8 is recommended, in order to benefit from lambda expressions that make code easier to write and more legible.

Fundamentals Hello Spark Hello Spark Hello Spark Hello Spark Hello Spark Hello Spark Hello Spark
The basic element is we will manipulate is the RDD: Resilient Distributed Dataset.

An RDD is a collection abstraction on which operations are performed in a distributed manner while being tolerant of hardware failures. The processing is written and seems to run in our JVM but it will be cut to run on multiple nodes. If a node fails, the sub-processing is automatically restarted on another node by the framework, without impacting the results.

The elements manipulated by the RDD (classes JavaRDD, JavaPairRDD…) can be simple classes (String, Integer…), your own custom classes, or, more frequently, tuples (Tuple2 class). In the latter case, the operations provided by the API will manipulate the collection as a key-value map.

The API exposed by the RDD can perform transformations on the data:
Hello world Hello world Hello world Hello world Hello world Hello world Hello world

map() can turn one element into another element
mapToPair() converts a key element to a tuple value
filter() allows you to filter elements by keeping only those that match an expression
flatMap() can cut an element into several others
reduce() and reduceByKey() allows the aggregation of elements together
etc.
These transformations are “lazy”: they will not run until a final operation is carried out at the end of the chain. Some of the final operations are:

count() to count the elements
collect() to retrieve the items in a collection in the Java JVM executor (dangerous in a cluster)
saveAsTextFile() to save the result into text files (see below)
etc.
Finally, the API can temporarily store an intermediate result with the methods cache() (memory storage) or persist() (storage in memory or on disk, depending on a parameter).

